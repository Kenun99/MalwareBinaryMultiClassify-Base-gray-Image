# coding:utf-8
import time
import matplotlib.pyplot as plt
import numpy as np
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.utils import to_categorical
from vector import GetVector
from evaluate import Metrics, print_res, multiclass_log_loss, plot_train, multi_class_report

from sklearn import metrics
from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix

# =================== configure =========================
epochs = 30
batchsize = 512
lr = 0.05
dropout_r = 0.1
Category = 9
layers_out = [256, 128, Category]

# ===================load data========================
train_data, train_label, test_data, test_label = GetVector()
tmp_test_label = test_label
train_label = to_categorical(train_label, Category)
test_label = to_categorical(test_label, Category)

print(train_data.shape, train_label.shape, test_data.shape, test_label.shape)


# ================== define model ================================
def bpnn(input_dim, layers_out, lr=0.001, dropout=0.1):
    model_name = 'bpnn'
    model = Sequential()
    model.add(Dense(layers_out[0], activation='relu', input_dim=input_dim, kernel_initializer='he_normal'))
    model.add(Dropout(dropout))
    model.add(Dense(layers_out[1], activation='relu', kernel_initializer='he_normal'))
    model.add(Dropout(dropout))
    model.add(Dense(layers_out[2], activation='softmax'))
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


input_dim = train_data.shape[1]
model = bpnn(input_dim, layers_out, lr=lr, dropout=dropout_r)
print(model.summary())

# =================== training =====================================
start_time = time.time()
hist = model.fit(train_data,
                 train_label,
                 batch_size=batchsize,
                 epochs=epochs)
print('Training duration: %d(s).' % (time.time() - start_time))
model.save('Bpnn_features10000_model_batchSize512')

# ===================== predict and  analysis ========================
plot_train(hist.history)  # print train loss and acc

prediction = model.predict(test_data)
score = model.evaluate(test_data, test_label)

print("loss:", score[0])
print("acc:", score[1])

M = Metrics(tmp_test_label, prediction)
M.describe()

'''
首先，对于一个测试样本：
1）标签只由0和1组成，
    1的位置表明了它的类别（可对应二分类问题中的‘正’），
    0就表示其他类别（‘负’）；
2）要是分类器对该测试样本分类正确，则该样本标签中1对应的位置在概率矩阵P中的值是大于0对应的位置的概率值的。

基于这两点，将标签矩阵L和概率矩阵P分别按行展开，转置后形成两列，这就得到了一个二分类的结果。
所以，此方法经过计算后可以直接得到最终的ROC曲线。
'''

np.save('test_prediction.npy', prediction)
np.save('test_label.pny', test_label)

y_one_hot = label_binarize(test_label, np.arange(Category))
y_score = model.predict_proba(test_data)
# macro 每次考虑一个类，求指标，最后求平均值， micro：通过先计算总体的TP，FN和FP的数量，再计算F1，直接得到指标
print('auc：', metrics.roc_auc_score(y_one_hot, y_score, average='micro'))  # 考虑标签平衡 通过先计算总体的TP，FN和FP的数量，再计算F1

tmp_prediction = [np.argmax(item) for item in prediction]
print("confusion_matrix:\n")
confusion_matrix = confusion_matrix(tmp_test_label, tmp_prediction)
plt.matshow(confusion_matrix, cmap=plt.cm.gray)
plt.savefig('./confusion_matrix.png')
print(confusion_matrix)

print('multiclass log loss: ', multiclass_log_loss(tmp_test_label, prediction))

tmp = [np.argmax(item) for item in prediction]
multi_class_report(tmp_test_label, tmp)

print("=========== Prediction result: ==========")

print_res(tmp)

'''
 =========== train data ==========
[+] Ramnit  :  1219
[+] Lollipop  :  1953
[+] Kelihos_ver3  :  2359
[+] Vundo  :  380
[+] Simda  :  38
[+] Tracur  :  610
[+] Kelihos_ver1  :  331
[+] Obfuscator.ACY  :  972
[+] Gatak  :  830
 =========== test data ==========
[+] Ramnit  :  223
[+] Lollipop  :  364
[+] Kelihos_ver3  :  411
[+] Vundo  :  72
[+] Simda  :  3
[+] Tracur  :  107
[+] Kelihos_ver1  :  47
[+] Obfuscator.ACY  :  178
[+] Gatak  :  129
(8692, 10000) (8692, 9) (1534, 10000) (1534, 9)

loss: 0.10448045153510811
acc: 0.970013037809648
precision= 0.8574345688692863
recall= 0.8576222157587683
F1_score= 0.8572350450376245
auc： 0.9992330512724188
confusion_matrix:

[[193   0   9   0   0   1   0   0   0]
 [  1 360   0   0   0   0   0   0   0]
 [  1   0 389   0   0   0   0   0   0]
 [  4   0   0  56   0   0   0   0   0]
 [  1   0   1   0   2   4   0   0   0]
 [  6   0   2   0   0 105   0   0   0]
 [  2   0  18   0   0   0  37   0   0]
 [  0   0   0   0   0   0   0 194   3]
 [  0   0   0   0   0   0   0   5 140]]
multiclass log loss:  0.10448045775659631

======================== Report: ============================
                precision    recall  f1-score   support

        Ramnit       0.93      0.95      0.94       203
      Lollipop       1.00      1.00      1.00       361
  Kelihos_ver3       0.93      1.00      0.96       390
         Vundo       1.00      0.93      0.97        60
         Simda       1.00      0.25      0.40         8
        Tracur       0.95      0.93      0.94       113
  Kelihos_ver1       1.00      0.65      0.79        57
Obfuscator.ACY       0.97      0.98      0.98       197
         Gatak       0.98      0.97      0.97       145

     micro avg       0.96      0.96      0.96      1534
     macro avg       0.97      0.85      0.88      1534
  weighted avg       0.96      0.96      0.96      1534


'''
